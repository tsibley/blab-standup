# 9 December 2020
# or, SFS ops wishlist


Premise: Invest in internal tooling to better surface our metrics and logs,
with the goals of reducing routine operational burden and improving incident
investigations.


## Grafana

[Grafana](https://grafana.com) is an operations dashboarding/viz tool that
integrates nicely with Prometheus.  As Metabase is to databases, Grafana is to
metrics (time series data).  It will give us persistent views into our metrics
and let us get away from bookmarking ad-hoc queries in Prometheus' basic web
console.  Grafana also integrates with other data sources, such as event log
storage systems (see below), so we can keep both operational metrics and logs
visibile in the same place.

We can install and operate it ourselves (managed via [infra][]/Ansible), like
we do with Prometheus.  There is likely to be very little maintenance required
once setup; mostly just occasional upgrades.  This will also make it possible
to use UW NetIDs for authn/authz.  I pushed a [WIP
commit](https://github.com/seattleflu/infra/commit/8f36d6d0e848900337be9f5bf9754b5e7ca45018)
for installing (but not configuring) it, authored back when I was doing the
Prometheus setup.

Alternatively, we could pay for the SaaS option: [Grafana
Cloud](https://grafana.com/products/cloud/).  The primary upside is obvious: we
don't have to operate/maintain it.  Downsides include base limits on the number
of time series (and additional charges above that) and no ability to use NetIDs
for authn/authz.  I suggest starting with self-hosted and moving to SaaS if
maintenance outstrips our resources.

### Configuration notes

The start of a config file from previous notes:

    instance_name = backoffice.seattleflu.org

    [server]
    http_addr = 127.0.0.1
    http_port = 3000
    root_url = https://backoffice.seattleflu.org/grafana/
    serve_from_sub_path = true

    [database]
    # Postgres on production (pointed to production RDS instance)
    # SQLite on dev VM

    [auth]
    disable_login_form = true
    signout_redirect_url = https://backoffice.seattleflu.org/Shibboleth.sso/Logout

    [auth.proxy]
    enabled = true
    header_name = Shibboleth-Remote-User
    header_property = username
    auto_sign_up = true
    whitelist = 127.0.0.1
    headers = Name:Shibboleth-cn Email:Shibboleth-mail
    enable_login_token = false

    [smtp]
    enabled = true
    from_address = grafana@backoffice.seattleflu.org

Apache will need to be configured to proxy to `http://localhost:3000/` and pass
the `Shibboleth-*` headers above (names are just examples) in a secure manner.

A Postgres database will need to be provisioned along with a `grafana` user,
similar to what we do for Metabase.


## Alerting

Setting up useful, informative alerts on metrics involved in key data processes
(ETLs, RoR, EHS reporting, web response times, etc) will let us see problems
sooner and, for some, even before they are a "problem", per se (e.g. ETL times
or response times steadily trending upwards).

Grafana supports alerting, and I think we should start there.  Particularly
with logs integrated into Grafana (see below), we can try to keep all/most
alerting defined in one spot.

Prometheus also includes an alerting component, Alertmanager, which is
generally considered more powerful/flexible than Grafana's alerts, but it is
likely more than we need initially.  Seems more prudent to start with the
simpler option.

Rule for alerts: must be actionable.  Ignorable alerts are ideally zero; at
very least must be minimized to all extents possible.


## New metrics

Tracking new metrics in Prometheus will help surface information that we
currently only look at if there's already a problem (sometimes much too late).
Some ideas:

  * `http_request_duration_seconds` summary metric (count + sum) for Apache, with
    the labels/dimensions `path` and `status`.

    This will let us see how long things wait in the Apache request queue and
    also see when error statuses start happening with more frequency (e.g. some
    service is down).

    Limit to certain paths to avoid time series bloat, e.g. exclude static
    assets.

    This could be parsed from the Apache logs, but other existing solutions
    probably exist too.  Due diligence to choose one or write our own simpler
    thing.

  * ID3C metrics, such as:

    - Counter metrics for rows in each receiving table (let's us see influx over time)
    - Gauge metrics for unprocessed rows in receiving tables (let's us see backlogs that happen)
    - Summary metrics for received â†’ processed duration in receiving tables (over past N days, let's us see backlogs and slowdowns)
    - Summary metrics for API routes (like we now have for Husky Musher; let's us see influx, timing, and errors)
    - ...

    These could be exported directly by ID3C's web API (built in metrics,
    nice!) or via a more generic tool like [sql_exporter](https://github.com/free/sql_exporter).

Many more possible metrics, for sure, but ok to start with just a couple.
We'll figure out what's missing as we go; the important thing is to have basic
framework in place so it's easy to add metrics when we realize what's missing.


## Event log store

Unified searching across all our logs.  Start with our existing, unstructured
logs; lots of benefit there still.  Then, over time, migrate more logs to
structured ones (JSON with specifics & context + human-readable description).

Kristen started looking into setting up an ELK stack?  I think we'd probably
want AWS SaaS versions of these, not self-hosted?  IME, the ELK stack in
particular has much more complex operational overhead than Prometheus/Grafana,
and the data volumes/rates are bigger.

Notably, Grafana can query/alert on ElasticSearch (the E in ELK).  Not sure if
we want the L & K (LogStash, Kibana) if we have Grafana; they overlap a little,
but not entirely, so probably still want them all.


## infra

Declaring more of our existing backoffice setup in [infra][] will help to make
individual app deploys repeatable as well as being able to rebuild the
backoffice itself more quickly/easily if necessary (see also the front/back
split proposed below).

### Declarative deploys

Declare (via Ansible playbooks) that version X of app A is the version that
should be deployed, and what that means in terms of files/dirs, service
restarts, etc.

To deploy a new version, change the declaration to version Y and re-run
Ansible.

### AWS resources

Currently we manage AWS resources (RDS instances, EC2 instances, S3 buckets,
IAM users/roles/policies, etc) manually via the web console.  We have a
small-enough number of these that it's very manageable.  We could work to move
that manual work to something declarative instead, like
[Pulumi](https://pulumi.com) (or Terraform, or CloudFormation, etc... although
Pulumi looks the nicest).

Note that this is a separate concern from the individual host config/app config
managed by Ansible.  Pulumi has a page describing this [conceptual
difference](https://www.pulumi.com/docs/intro/vs/chef_puppet_etc/).


## Separate frontoffice/backoffice hosts

Split external-facing services on the frontoffice and internal ones are the
backoffice.  Separation for two primary reasons:

 1. Insulate public services from internal services so issues with e.g. Musher
    don't impact Switchboard.

 2. Security barrier between web-accessible services (external or internal) from
    batch jobs (ETLs, RoR, etc).  For example, batch jobs generally have
    greater access to data than web-accessible services, and isolating them can
    help avoid accidental leaks.

Taken together, this maybe implies a three-way split instead of two-way split:

 1. Public-facing web services.
 2. Internal-facing web services.
 3. Internal processes.

Declaring more of our existing backoffice setup in [infra][] will help to
eventually point those declarations at different hosts instead.  It will also,
in the meantime, make deploys more push-button.


[infra]: https://github.com/seattleflu/infra
