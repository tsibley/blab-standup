# 14 July 2022
# or, a debugging breakthrough, finally?

_following up on past two days of digging from [2022-07-12](2022-07-12.txt)_

With fresh eyes this morning and eyeballing the `DEBUG="*"` logs from the
canary some more, I was able to consistently reproduce the problem!

For a request to fail with the 503 (H18), it must:

  - go thru the handlers in src/endpoints/sources.js, e.g. via the RESTful API
    or the Charon API since 62d17e1.

  - have a valid session cookie; requests without a session are unaffected (!)

  - be conditioned by valid If-Modified-Since and/or If-None-Match headers such
    that a 304 is expected.

On the server side, it's also possible the local fetch() cache needs to be
populated first (e.g. requests only exhibit the error on cache revalidation
instead of a cache miss), but I haven't confirmed that yet.  Update: Nope, not
necessary.

Not sure what the ultimate cause is, but the proximate cause appears to be that
on the Express routing/handler stack, `next()` is being called after the
response should already be finished (e.g. the request has already hit a
terminal handler).  This abnormally continues the request processing onto
further routes, to the point of passing thru

    app.use(endpoints.static.gatsbyAssets);
    
and then our final

    app.use((req, res, next) => next(new NotFound()))
    
and ending up in our error handler.

What is calling `next()` is not yet clear, but `@awaitjs/express`'s handling of
`next()` is suspect; see [previous notes](2022-07-12.txt) as well as [bug
1](https://github.com/vkarpov15/awaitjs-express/pull/23/files) and [bug
2](https://github.com/vkarpov15/awaitjs-express/pull/27/files).  We're running
0.6.3 and while bug 1 was introduced in 0.7.1 (then fixed (maybe) in 0.7.2),
bug 2 was fixed in 0.8.0 but was present long before 0.6.3.  When reading the
code, I also noted that it can swallow errors if the headers have already been
sent:

    } catch(err) {
      // trs: should unconditionally call next(err) to pass it up the chain
      res.headersSent ? null : next(err);
    }

Given all the above, even if `@awaitjs/express` isn't actually at fault here,
I'd like to replace it with our own simpler, more correct wrapper instead.  I
think a simpler wrapper should be straightforward; all the
complexity/difficulties in `@awaitjs/express`'s impl. are because they're
trying to change the model of request flow in Express to not require async
middleware functions to call `next()`, but that's not something we need/want.
It wasn't clear when adopting this module to make async handlers work well that
it also changed the request flow model.

Anyway, there's more to debug here to understand why this is happening, but it
should be much easier with a reliable repro.

I captured server-side `DEBUG="*"` logs from reproing against the RESTful and
Charon APIs:

  - [2022-07-14-repro-restful-api.log](2022-07-14-repro-restful-api.log)
  - [2022-07-14-repro-charon-api.log](2022-07-14-repro-charon-api.log)

Note that the files contain ANSI escapes as emitted by `heroku logs --color`,
so they're best viewed with `less -R`.

Each log is sorted by the timestamp (with `sort -k1.6,1` to ignore leading ANSI
escapes on each line), unlike the original `heroku logs` output, and chunked
into conceptual stanzas (which happens to also be by timestamp second).  The
three main request stanzas correspond to requests made with:

  1. Valid cookie but invalid If-None-Match header (e.g. 304 not expected); succeeds.
  2. Valid cookie and valid If-None-Match header (e.g. 304 expected); fails.
  3. No cookie but valid If-None-Match header (e.g. 304 expected); succeeds.

With the details above uncovered, I'm now also able to repro this locally with
a request like:

    $ curl -sv --compressed -H'Cookie: nextstrain.org-session=MASKED' -H'If-None-Match: "55330901951446270d9ef07346672385"' -H'Accept: application/json' 'localhost:5000/charon/getDataset?prefix=enterovirus/d68/vp1' 
    *   Trying 127.0.0.1...
    * TCP_NODELAY set
    * Connected to localhost (127.0.0.1) port 5000 (#0)
    > GET /charon/getDataset?prefix=enterovirus/d68/vp1 HTTP/1.1
    > Host: localhost:5000
    > User-Agent: curl/7.58.0
    > Accept-Encoding: deflate, gzip
    > Cookie: nextstrain.org-session=MASKED
    > If-None-Match: "55330901951446270d9ef07346672385"
    > Accept: application/json
    > 
    < HTTP/1.1 404 Not Found
    < X-Powered-By: Express
    < Access-Control-Allow-Origin: http://localhost:8000
    < Vary: Origin, Accept, Accept-Encoding
    < ETag: "55330901951446270d9ef07346672385"
    < Last-Modified: Tue, 31 May 2022 14:52:13 GMT
    < Content-Type: text/html; charset=utf-8
    < Content-Length: 156
    < Content-Security-Policy: default-src 'none'
    < X-Content-Type-Options: nosniff
    < Set-Cookie: nextstrain.org-session=MASKED; Path=/; Expires=Sat, 13 Aug 2022 19:03:32 GMT; HttpOnly; SameSite=Lax
    < Date: Thu, 14 Jul 2022 19:03:32 GMT
    < Connection: keep-alive
    < Keep-Alive: timeout=5
    < 
    * transfer closed with 156 bytes remaining to read
    * stopped the pause stream!
    * Closing connection 0

The local server-side logs look much like the Heroku server-side logs, although
there's some weirdness with the relative timestamps output at the end of each
line with some reporting wildly long times (`+14s`, `+1m`!) that appear
incorrect.  I'd hope this is a benign bug and not indicative of timestamps
getting mixed up/carried over between requests, as that might indicate more
request context is mixed up/carried over too.  Update: looks like it might
indeed be benign, as these abnormal durations match that reported by the
initial `express:router dispatching GET …` log line, implying that perhaps the
producers are reporting from request start instead of previous log line start.
Not sure exactly where these durations are calculated at the moment.

Raw logs (different formatting than to the console) are in
[2022-07-14-repro-charon-api-local.log](2022-07-14-repro-charon-api-local.log).

Notably, there's a ~6s delay between curl receiving the response headers and
the connection being closed (without any bytes sent).  This lines up with the
(default) `agentkeepalive` timeout of seemingly 6s closing the upstream
connection that's been piped into the client/curl connection:

    agentkeepalive sock[14#nextstrain-data.s3.amazonaws.com:443::::::::true:::::::::::::](requests: 1, finished: 1) close, isError: false +6s

So while I don't know if this has anything to do with the 503s, it does seem
like we should stop the `pipeline(upstreamRes, res)` call once the response
data has been read, e.g. before the keep-alive timeout.  Possibly just
disabling keep-alive for now!

## Later that afternoon…

Yup, it's `@awaitjs/express`.

We call

    return res.status(304).end();

from deep within our `getDataset` handler (`fn` below) wrapped by the library, returning into it:

    try {
      await fn(req, res, next);
      // --> after return execution is here <--
      res.headersSent ? null : next();
    } catch(err) {
      res.headersSent ? null : next(err);
    }

This races on `res.headersSent` and when it isn't true then `next()` is called.
This is counter to Express' model where `next()` is never implicit but always
explicit, and the request stalls if `next()` is not called and no response is
written.

Things happen to work out when a session isn't involved, presumably because the
session handlers need to do some end-of-response work to touch the cookie and
that interferes with the confused processing of the existing/pending response.

One workaround is to wait explicitly for `.end()` to complete before returning,
something like:

    const finished = promisify(stream.finished);
    ...
    return await finished(res.status(304).end());

or:

    return await new Promise((resolve, reject) =>
      res.status(304).end(err => err ? reject(err) : resolve()));

but all this does is paper over the bug in `@awaitjs/express` so it always sees
`res.headersSent === true`.

I'm going to fix (well, replace) `@awaitjs/express` with a correct minimal
implementation instead.
